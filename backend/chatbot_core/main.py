# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MBZ98A1AEtSLkXlRaHxO4rEDW2NKSlQJ
"""

api = "AIzaSyDKHuhYjgJNXFdHNoYmzJQZkSB5433HEBk"

import pandas as pd
df = pd.read_csv('output.csv')
df.head(3)

text = df['content'].str.cat(sep=' ')
len(text)
type(text)
print(text)

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings ,ChatOpenAI
from langchain.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
import faiss
import numpy as np
from langchain.embeddings import HuggingFaceEmbeddings
# from langchain.chains.question_answering import load_qa_chain



# making chunks of documents so that search esily and effectively
splitter  = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
# docs = splitter.split_text(text)
chunks = splitter.create_documents([text])

chunk_texts = [chunk.page_content for chunk in chunks]

print(chunk_texts)

# embedding_model_name = 'all-MiniLM-L6-v2'\
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
# try:
#     embedding_model = SentenceTransformer(embedding_model_name)
#     print(f"Embedding model '{embedding_model_name}' loaded.")
# except Exception as e:
#     print(f"Error loading embedding model: {e}")
#     print("Please ensure you have an internet connection or the model is cached.")
#     print("Consider installing `sentence-transformers` via `pip install sentence-transformers`.")
#     exit() # Exit if we can't load the model

# Extract text content from Document objects

# Generate embeddings for all chunks
# print(f"Generating embeddings for {len(chunk_texts)} chunks...")
# chunk_embeddings = embedding_model.encode(chunk_texts, show_progress_bar=True,convert_to_numpy=True)
# embeddings = normalize(chunk_embeddings)
# dimension = embeddings.shape[1]
# index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity
# index.add(embeddings)
# print(index)\
vectorstore = FAISS.from_texts(chunk_texts, embedding_model)


# print(f"Embeddings generated. Shape: {chunk_embeddings.shape}") # Should be (num_chunks, embedding_dimension)

# print(index[2]
print(vectorstore.get_by_ids([vectorstore.index_to_docstore_id[0]]))

#Now we are making retriver whihc will retriev infor / chunks from vector database
retriever = vectorstore.as_retriever(search_kwargs={"k": 4},search_type="similarity") # what we need is 4 search chunks

retriever.invoke('Give me report on pakistans punjab wheather condition')



from langchain_google_genai import ChatGoogleGenerativeAI

# Initialize the LLM
# You can specify different Gemini models, e.g., "gemini-pro", "gemini-1.5-flash", "gemini-1.5-pro"
# "gemini-1.5-flash" is often a good balance of speed and capability for RAG.
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",  # Or "gemini-pro", "gemini-1.5-pro"
    temperature=0.7,          # Controls creativity/randomness (0.0 to 1.0)
    # You can also pass google_api_key=os.getenv("GOOGLE_API_KEY") directly
    # if you don't want to set it as an environment variable globally.
    api_key=api
)

print("Gemini LLM initialized with LangChain.")

promt = PromptTemplate(
    template = """
    You are a helpfull assistant
    Answer only from the provided concept.
    If the context is innsufficient just say that you are not caught up with that news or you dont know.
    {context}
    Question: {question}

    """,
    input_variables=['context','question']

)
question = "Give me report on pakistans punjab wheather condition"
retriveddocs = retriever.invoke(question)

question = 'what do you say about atom bomb dropped on hiroshima nangasaki'
ret_docs = retriever.invoke(question)

context_text = '\n\n'.join(doc.page_content for doc in ret_docs)

final = promt.invoke({'context':context_text,'question':question})
answer = llm.invoke(final)
print(answer.content)